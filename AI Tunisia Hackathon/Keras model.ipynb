{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_train = pd.read_csv('UnScaled Data_train.csv')\n",
    "read_test = pd.read_csv('UnScaled Data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>location_type</th>\n",
       "      <th>cellphone_access</th>\n",
       "      <th>household_size</th>\n",
       "      <th>age_of_respondent</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>education_level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>country_Kenya</th>\n",
       "      <th>...</th>\n",
       "      <th>country_Tanzania</th>\n",
       "      <th>country_Uganda</th>\n",
       "      <th>gender_of_respondent_Female</th>\n",
       "      <th>gender_of_respondent_Male</th>\n",
       "      <th>relationship_with_head_Child</th>\n",
       "      <th>relationship_with_head_Head of Household</th>\n",
       "      <th>relationship_with_head_Other non-relatives</th>\n",
       "      <th>relationship_with_head_Other relative</th>\n",
       "      <th>relationship_with_head_Parent</th>\n",
       "      <th>relationship_with_head_Spouse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year  location_type  cellphone_access  household_size  \\\n",
       "0           0     2              0                 1               2   \n",
       "1           1     2              0                 0               4   \n",
       "2           2     2              1                 1               4   \n",
       "3           3     2              0                 1               4   \n",
       "4           4     2              1                 0               7   \n",
       "\n",
       "   age_of_respondent  marital_status  education_level  job_type  \\\n",
       "0                  8               2                3         9   \n",
       "1                 54               4                0         4   \n",
       "2                 10               3                5         9   \n",
       "3                 18               2                2         3   \n",
       "4                 10               3                2         5   \n",
       "\n",
       "   country_Kenya  ...  country_Tanzania  country_Uganda  \\\n",
       "0              1  ...                 0               0   \n",
       "1              1  ...                 0               0   \n",
       "2              1  ...                 0               0   \n",
       "3              1  ...                 0               0   \n",
       "4              1  ...                 0               0   \n",
       "\n",
       "   gender_of_respondent_Female  gender_of_respondent_Male  \\\n",
       "0                            1                          0   \n",
       "1                            1                          0   \n",
       "2                            0                          1   \n",
       "3                            1                          0   \n",
       "4                            0                          1   \n",
       "\n",
       "   relationship_with_head_Child  relationship_with_head_Head of Household  \\\n",
       "0                             0                                         0   \n",
       "1                             0                                         1   \n",
       "2                             0                                         0   \n",
       "3                             0                                         1   \n",
       "4                             1                                         0   \n",
       "\n",
       "   relationship_with_head_Other non-relatives  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "\n",
       "   relationship_with_head_Other relative  relationship_with_head_Parent  \\\n",
       "0                                      0                              0   \n",
       "1                                      0                              0   \n",
       "2                                      1                              0   \n",
       "3                                      0                              0   \n",
       "4                                      0                              0   \n",
       "\n",
       "   relationship_with_head_Spouse  \n",
       "0                              1  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_train = read_train.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23524, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10086, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_test = read_test.drop('Unnamed: 0', axis = 1)\n",
    "read_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23524,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv('train_target.csv')\n",
    "target = target.drop('0', axis = 1)\n",
    "update_target= [1]\n",
    "\n",
    "for k in target['1']:\n",
    "    update_target.append(k)\n",
    "targ = np.array(update_target)\n",
    "targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target = target.astype('int')\n",
    "x_train, x_test,y_train, y_test = train_test_split(read_train, targ, train_size = 0.8, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build keras model\n",
    "\n",
    "from keras import models, Sequential, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1000)              101000    \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,204,301\n",
      "Trainable params: 1,204,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(100, activation = 'relu', input_shape = (20,)))\n",
    "model.add(layers.Dense(1000, activation = 'relu'))\n",
    "model.add(layers.Dense(1000, activation = 'relu'))\n",
    "model.add(layers.Dense(100, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model prediction on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18819 samples, validate on 4705 samples\n",
      "Epoch 1/30\n",
      "18819/18819 [==============================] - 3s 143us/step - loss: 0.4172 - acc: 0.8581 - val_loss: 0.3972 - val_acc: 0.8646\n",
      "Epoch 2/30\n",
      "18819/18819 [==============================] - 2s 95us/step - loss: 0.3923 - acc: 0.8611 - val_loss: 0.3772 - val_acc: 0.8648\n",
      "Epoch 3/30\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 0.4029 - acc: 0.8592 - val_loss: 0.3557 - val_acc: 0.8687\n",
      "Epoch 4/30\n",
      "18819/18819 [==============================] - 2s 96us/step - loss: 0.3601 - acc: 0.8669 - val_loss: 0.3434 - val_acc: 0.8755\n",
      "Epoch 5/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 0.3584 - acc: 0.8676 - val_loss: 0.3372 - val_acc: 0.8774\n",
      "Epoch 6/30\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 0.3467 - acc: 0.8701 - val_loss: 0.3296 - val_acc: 0.8769\n",
      "Epoch 7/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.3477 - acc: 0.8701 - val_loss: 0.3260 - val_acc: 0.8765\n",
      "Epoch 8/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3437 - acc: 0.8707 - val_loss: 0.3430 - val_acc: 0.8740\n",
      "Epoch 9/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3425 - acc: 0.8700 - val_loss: 0.3209 - val_acc: 0.8780\n",
      "Epoch 10/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 0.3355 - acc: 0.8720 - val_loss: 0.3185 - val_acc: 0.8778\n",
      "Epoch 11/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 0.3289 - acc: 0.8732 - val_loss: 0.3659 - val_acc: 0.8691\n",
      "Epoch 12/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3298 - acc: 0.8748 - val_loss: 0.3546 - val_acc: 0.8701\n",
      "Epoch 13/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3284 - acc: 0.8740 - val_loss: 0.3481 - val_acc: 0.8710\n",
      "Epoch 14/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 0.3298 - acc: 0.8748 - val_loss: 0.3094 - val_acc: 0.8806\n",
      "Epoch 15/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 0.3228 - acc: 0.8745 - val_loss: 0.3397 - val_acc: 0.8716\n",
      "Epoch 16/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.3221 - acc: 0.8750 - val_loss: 0.3204 - val_acc: 0.8750\n",
      "Epoch 17/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.3269 - acc: 0.8737 - val_loss: 0.3067 - val_acc: 0.8793\n",
      "Epoch 18/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3165 - acc: 0.8771 - val_loss: 0.3098 - val_acc: 0.8799\n",
      "Epoch 19/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 0.3160 - acc: 0.8770 - val_loss: 0.3057 - val_acc: 0.8799\n",
      "Epoch 20/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.3238 - acc: 0.8748 - val_loss: 0.3044 - val_acc: 0.8801\n",
      "Epoch 21/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 0.3147 - acc: 0.8768 - val_loss: 0.3015 - val_acc: 0.8835\n",
      "Epoch 22/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3145 - acc: 0.8774 - val_loss: 0.3055 - val_acc: 0.8827\n",
      "Epoch 23/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3186 - acc: 0.8765 - val_loss: 0.3058 - val_acc: 0.8820\n",
      "Epoch 24/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3148 - acc: 0.8773 - val_loss: 0.3221 - val_acc: 0.8812\n",
      "Epoch 25/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.3118 - acc: 0.8768 - val_loss: 0.2965 - val_acc: 0.8829\n",
      "Epoch 26/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.3105 - acc: 0.8775 - val_loss: 0.2956 - val_acc: 0.8814\n",
      "Epoch 27/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.3178 - acc: 0.8762 - val_loss: 0.2990 - val_acc: 0.8812\n",
      "Epoch 28/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3094 - acc: 0.8783 - val_loss: 0.2977 - val_acc: 0.8825\n",
      "Epoch 29/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3152 - acc: 0.8759 - val_loss: 0.2992 - val_acc: 0.8818\n",
      "Epoch 30/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.3051 - acc: 0.8800 - val_loss: 0.2956 - val_acc: 0.8848\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = ['acc'])\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['acc'])\n",
    "\n",
    "#ada grad\n",
    "#ada_grad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 30, batch_size = 1000, \n",
    "                   validation_data = (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18819 samples, validate on 4705 samples\n",
      "Epoch 1/30\n",
      "18819/18819 [==============================] - 3s 147us/step - loss: 2.1981 - acc: 0.8411 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 2/30\n",
      "18819/18819 [==============================] - 2s 95us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 3/30\n",
      "18819/18819 [==============================] - 2s 95us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 4/30\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 5/30\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 6/30\n",
      "18819/18819 [==============================] - 2s 96us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 7/30\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 8/30\n",
      "18819/18819 [==============================] - 2s 98us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 9/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 10/30\n",
      "18819/18819 [==============================] - 2s 96us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 11/30\n",
      "18819/18819 [==============================] - 2s 95us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 12/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 13/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 14/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 15/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 16/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 17/30\n",
      "18819/18819 [==============================] - 2s 104us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 18/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 19/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 20/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 21/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 22/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 23/30\n",
      "18819/18819 [==============================] - 2s 98us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 24/30\n",
      "18819/18819 [==============================] - 2s 98us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 25/30\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 26/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 27/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 28/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 29/30\n",
      "18819/18819 [==============================] - 2s 98us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 30/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = ['acc'])\n",
    "#ada grad\n",
    "ada_grad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "#ada_delta\n",
    "#ada_delta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = ada_grad, metrics = ['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 30, batch_size = 1000, \n",
    "                   validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18819 samples, validate on 4705 samples\n",
      "Epoch 1/30\n",
      "18819/18819 [==============================] - 3s 165us/step - loss: 0.5041 - acc: 0.8329 - val_loss: 0.3846 - val_acc: 0.8635\n",
      "Epoch 2/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.4090 - acc: 0.8581 - val_loss: 0.4000 - val_acc: 0.8635\n",
      "Epoch 3/30\n",
      "18819/18819 [==============================] - 2s 107us/step - loss: 0.3947 - acc: 0.8587 - val_loss: 0.3795 - val_acc: 0.8635\n",
      "Epoch 4/30\n",
      "18819/18819 [==============================] - 2s 108us/step - loss: 0.3867 - acc: 0.8593 - val_loss: 0.3690 - val_acc: 0.8638\n",
      "Epoch 5/30\n",
      "18819/18819 [==============================] - 2s 109us/step - loss: 0.3762 - acc: 0.8629 - val_loss: 0.4080 - val_acc: 0.8638\n",
      "Epoch 6/30\n",
      "18819/18819 [==============================] - 2s 112us/step - loss: 0.3712 - acc: 0.8634 - val_loss: 0.3736 - val_acc: 0.8663\n",
      "Epoch 7/30\n",
      "18819/18819 [==============================] - 2s 112us/step - loss: 0.3635 - acc: 0.8664 - val_loss: 0.3372 - val_acc: 0.8697\n",
      "Epoch 8/30\n",
      "18819/18819 [==============================] - 2s 112us/step - loss: 0.3498 - acc: 0.8669 - val_loss: 0.3337 - val_acc: 0.8706\n",
      "Epoch 9/30\n",
      "18819/18819 [==============================] - 2s 115us/step - loss: 0.3520 - acc: 0.8668 - val_loss: 0.3235 - val_acc: 0.8742\n",
      "Epoch 10/30\n",
      "18819/18819 [==============================] - 2s 111us/step - loss: 0.3472 - acc: 0.8702 - val_loss: 0.3262 - val_acc: 0.8740\n",
      "Epoch 11/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 0.3381 - acc: 0.8705 - val_loss: 0.3244 - val_acc: 0.8774\n",
      "Epoch 12/30\n",
      "18819/18819 [==============================] - 2s 112us/step - loss: 0.3421 - acc: 0.8709 - val_loss: 0.3195 - val_acc: 0.8795\n",
      "Epoch 13/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 0.3394 - acc: 0.8717 - val_loss: 0.3151 - val_acc: 0.8795\n",
      "Epoch 14/30\n",
      "18819/18819 [==============================] - 2s 109us/step - loss: 0.3326 - acc: 0.8723 - val_loss: 0.3129 - val_acc: 0.8772\n",
      "Epoch 15/30\n",
      "18819/18819 [==============================] - 2s 109us/step - loss: 0.3329 - acc: 0.8732 - val_loss: 0.3097 - val_acc: 0.8814\n",
      "Epoch 16/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 0.3384 - acc: 0.8703 - val_loss: 0.3203 - val_acc: 0.8744\n",
      "Epoch 17/30\n",
      "18819/18819 [==============================] - 2s 112us/step - loss: 0.3255 - acc: 0.8742 - val_loss: 0.3171 - val_acc: 0.8735\n",
      "Epoch 18/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 0.3264 - acc: 0.8737 - val_loss: 0.3045 - val_acc: 0.8774\n",
      "Epoch 19/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 0.3204 - acc: 0.8751 - val_loss: 0.3192 - val_acc: 0.8778\n",
      "Epoch 20/30\n",
      "18819/18819 [==============================] - 2s 111us/step - loss: 0.3222 - acc: 0.8742 - val_loss: 0.3067 - val_acc: 0.8827\n",
      "Epoch 21/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 0.3226 - acc: 0.8755 - val_loss: 0.3024 - val_acc: 0.8825\n",
      "Epoch 22/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 0.3227 - acc: 0.8762 - val_loss: 0.3038 - val_acc: 0.8795\n",
      "Epoch 23/30\n",
      "18819/18819 [==============================] - 2s 115us/step - loss: 0.3150 - acc: 0.8760 - val_loss: 0.3070 - val_acc: 0.8761\n",
      "Epoch 24/30\n",
      "18819/18819 [==============================] - 2s 119us/step - loss: 0.3141 - acc: 0.8758 - val_loss: 0.3137 - val_acc: 0.8829\n",
      "Epoch 25/30\n",
      "18819/18819 [==============================] - 2s 120us/step - loss: 0.3088 - acc: 0.8781 - val_loss: 0.3058 - val_acc: 0.8814\n",
      "Epoch 26/30\n",
      "18819/18819 [==============================] - 2s 114us/step - loss: 0.3103 - acc: 0.8784 - val_loss: 0.3314 - val_acc: 0.8678\n",
      "Epoch 27/30\n",
      "18819/18819 [==============================] - 2s 113us/step - loss: 0.3124 - acc: 0.8769 - val_loss: 0.2942 - val_acc: 0.8842\n",
      "Epoch 28/30\n",
      "18819/18819 [==============================] - 2s 111us/step - loss: 0.3167 - acc: 0.8765 - val_loss: 0.2994 - val_acc: 0.8801\n",
      "Epoch 29/30\n",
      "18819/18819 [==============================] - 2s 113us/step - loss: 0.3052 - acc: 0.8794 - val_loss: 0.2912 - val_acc: 0.8835\n",
      "Epoch 30/30\n",
      "18819/18819 [==============================] - 2s 116us/step - loss: 0.3037 - acc: 0.8798 - val_loss: 0.2992 - val_acc: 0.8797\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = ['acc'])\n",
    "#ada grad\n",
    "#ada_grad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "#ada_delta\n",
    "ada_delta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = ada_delta, metrics = ['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 30, batch_size = 1000, \n",
    "                   validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18819 samples, validate on 4705 samples\n",
      "Epoch 1/30\n",
      "18819/18819 [==============================] - 3s 185us/step - loss: 0.4841 - acc: 0.8274 - val_loss: 0.3748 - val_acc: 0.8635\n",
      "Epoch 2/30\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 0.3634 - acc: 0.8623 - val_loss: 0.3450 - val_acc: 0.8718\n",
      "Epoch 3/30\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 0.3448 - acc: 0.8667 - val_loss: 0.3405 - val_acc: 0.8682\n",
      "Epoch 4/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 0.3303 - acc: 0.8713 - val_loss: 0.3161 - val_acc: 0.8778\n",
      "Epoch 5/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.3213 - acc: 0.8739 - val_loss: 0.3103 - val_acc: 0.8797\n",
      "Epoch 6/30\n",
      "18819/18819 [==============================] - 2s 108us/step - loss: 0.3134 - acc: 0.8766 - val_loss: 0.3013 - val_acc: 0.8827\n",
      "Epoch 7/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.3123 - acc: 0.8773 - val_loss: 0.3013 - val_acc: 0.8812\n",
      "Epoch 8/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3089 - acc: 0.8773 - val_loss: 0.2962 - val_acc: 0.8795\n",
      "Epoch 9/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 0.3076 - acc: 0.8791 - val_loss: 0.2938 - val_acc: 0.8852\n",
      "Epoch 10/30\n",
      "18819/18819 [==============================] - 2s 100us/step - loss: 0.3103 - acc: 0.8774 - val_loss: 0.3058 - val_acc: 0.8812\n",
      "Epoch 11/30\n",
      "18819/18819 [==============================] - 2s 101us/step - loss: 0.3026 - acc: 0.8796 - val_loss: 0.3058 - val_acc: 0.8795\n",
      "Epoch 12/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.2993 - acc: 0.8796 - val_loss: 0.2887 - val_acc: 0.8850\n",
      "Epoch 13/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.2943 - acc: 0.8816 - val_loss: 0.2838 - val_acc: 0.8874\n",
      "Epoch 14/30\n",
      "18819/18819 [==============================] - 2s 113us/step - loss: 0.2986 - acc: 0.8795 - val_loss: 0.2983 - val_acc: 0.8846\n",
      "Epoch 15/30\n",
      "18819/18819 [==============================] - 2s 109us/step - loss: 0.2970 - acc: 0.8807 - val_loss: 0.2984 - val_acc: 0.8837\n",
      "Epoch 16/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.2949 - acc: 0.8820 - val_loss: 0.2987 - val_acc: 0.8842\n",
      "Epoch 17/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 0.2991 - acc: 0.8797 - val_loss: 0.2923 - val_acc: 0.8857\n",
      "Epoch 18/30\n",
      "18819/18819 [==============================] - 2s 108us/step - loss: 0.2891 - acc: 0.8814 - val_loss: 0.2879 - val_acc: 0.8854\n",
      "Epoch 19/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.2898 - acc: 0.8814 - val_loss: 0.2864 - val_acc: 0.8842\n",
      "Epoch 20/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.2848 - acc: 0.8819 - val_loss: 0.2824 - val_acc: 0.8857\n",
      "Epoch 21/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 0.2846 - acc: 0.8830 - val_loss: 0.2870 - val_acc: 0.8882\n",
      "Epoch 22/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.2840 - acc: 0.8833 - val_loss: 0.2820 - val_acc: 0.8878\n",
      "Epoch 23/30\n",
      "18819/18819 [==============================] - 2s 108us/step - loss: 0.2853 - acc: 0.8816 - val_loss: 0.2966 - val_acc: 0.8859\n",
      "Epoch 24/30\n",
      "18819/18819 [==============================] - 2s 107us/step - loss: 0.2817 - acc: 0.8842 - val_loss: 0.2820 - val_acc: 0.8882\n",
      "Epoch 25/30\n",
      "18819/18819 [==============================] - 2s 119us/step - loss: 0.2815 - acc: 0.8852 - val_loss: 0.2851 - val_acc: 0.8882\n",
      "Epoch 26/30\n",
      "18819/18819 [==============================] - 2s 125us/step - loss: 0.2871 - acc: 0.8834 - val_loss: 0.2847 - val_acc: 0.8867\n",
      "Epoch 27/30\n",
      "18819/18819 [==============================] - 2s 120us/step - loss: 0.2783 - acc: 0.8849 - val_loss: 0.2819 - val_acc: 0.8882\n",
      "Epoch 28/30\n",
      "18819/18819 [==============================] - 2s 120us/step - loss: 0.2804 - acc: 0.8851 - val_loss: 0.2966 - val_acc: 0.8837\n",
      "Epoch 29/30\n",
      "18819/18819 [==============================] - 2s 124us/step - loss: 0.2794 - acc: 0.8853 - val_loss: 0.2875 - val_acc: 0.8867\n",
      "Epoch 30/30\n",
      "18819/18819 [==============================] - 2s 125us/step - loss: 0.2823 - acc: 0.8852 - val_loss: 0.2842 - val_acc: 0.8874\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = ['acc'])\n",
    "#ada grad\n",
    "#ada_grad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "#ada_delta\n",
    "#ada_delta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "#Adam\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 30, batch_size = 1000, \n",
    "                   validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18819 samples, validate on 4705 samples\n",
      "Epoch 1/30\n",
      "18819/18819 [==============================] - 3s 162us/step - loss: 0.4928 - acc: 0.8528 - val_loss: 0.3769 - val_acc: 0.8635\n",
      "Epoch 2/30\n",
      "18819/18819 [==============================] - 2s 99us/step - loss: 0.3688 - acc: 0.8596 - val_loss: 0.3427 - val_acc: 0.8706\n",
      "Epoch 3/30\n",
      "18819/18819 [==============================] - 2s 108us/step - loss: 0.3457 - acc: 0.8670 - val_loss: 0.3304 - val_acc: 0.8733\n",
      "Epoch 4/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.3362 - acc: 0.8707 - val_loss: 0.3195 - val_acc: 0.8780\n",
      "Epoch 5/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3267 - acc: 0.8735 - val_loss: 0.3167 - val_acc: 0.8784\n",
      "Epoch 6/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3210 - acc: 0.8742 - val_loss: 0.3084 - val_acc: 0.8801\n",
      "Epoch 7/30\n",
      "18819/18819 [==============================] - 2s 109us/step - loss: 0.3136 - acc: 0.8766 - val_loss: 0.3040 - val_acc: 0.8806\n",
      "Epoch 8/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3105 - acc: 0.8774 - val_loss: 0.3069 - val_acc: 0.8793\n",
      "Epoch 9/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.3172 - acc: 0.8752 - val_loss: 0.3020 - val_acc: 0.8795\n",
      "Epoch 10/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.3091 - acc: 0.8782 - val_loss: 0.2960 - val_acc: 0.8818\n",
      "Epoch 11/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.3019 - acc: 0.8791 - val_loss: 0.2977 - val_acc: 0.8806\n",
      "Epoch 12/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.3021 - acc: 0.8782 - val_loss: 0.2940 - val_acc: 0.8844\n",
      "Epoch 13/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.3006 - acc: 0.8782 - val_loss: 0.2958 - val_acc: 0.8831\n",
      "Epoch 14/30\n",
      "18819/18819 [==============================] - 2s 104us/step - loss: 0.2978 - acc: 0.8807 - val_loss: 0.2889 - val_acc: 0.8825\n",
      "Epoch 15/30\n",
      "18819/18819 [==============================] - 2s 104us/step - loss: 0.2999 - acc: 0.8789 - val_loss: 0.2986 - val_acc: 0.8840\n",
      "Epoch 16/30\n",
      "18819/18819 [==============================] - 2s 102us/step - loss: 0.2994 - acc: 0.8806 - val_loss: 0.2963 - val_acc: 0.8820\n",
      "Epoch 17/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.2956 - acc: 0.8804 - val_loss: 0.2984 - val_acc: 0.8829\n",
      "Epoch 18/30\n",
      "18819/18819 [==============================] - 2s 104us/step - loss: 0.2939 - acc: 0.8817 - val_loss: 0.2900 - val_acc: 0.8844\n",
      "Epoch 19/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.2976 - acc: 0.8790 - val_loss: 0.2864 - val_acc: 0.8842\n",
      "Epoch 20/30\n",
      "18819/18819 [==============================] - 2s 104us/step - loss: 0.2936 - acc: 0.8812 - val_loss: 0.2935 - val_acc: 0.8825\n",
      "Epoch 21/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.2903 - acc: 0.8819 - val_loss: 0.2933 - val_acc: 0.8825\n",
      "Epoch 22/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.2886 - acc: 0.8819 - val_loss: 0.2873 - val_acc: 0.8842\n",
      "Epoch 23/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.2910 - acc: 0.8818 - val_loss: 0.2886 - val_acc: 0.8831\n",
      "Epoch 24/30\n",
      "18819/18819 [==============================] - 2s 107us/step - loss: 0.2906 - acc: 0.8821 - val_loss: 0.2876 - val_acc: 0.8837\n",
      "Epoch 25/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.2891 - acc: 0.8813 - val_loss: 0.2853 - val_acc: 0.8867\n",
      "Epoch 26/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.2866 - acc: 0.8833 - val_loss: 0.2864 - val_acc: 0.8846\n",
      "Epoch 27/30\n",
      "18819/18819 [==============================] - 2s 104us/step - loss: 0.2878 - acc: 0.8827 - val_loss: 0.2853 - val_acc: 0.8848\n",
      "Epoch 28/30\n",
      "18819/18819 [==============================] - 2s 108us/step - loss: 0.2862 - acc: 0.8834 - val_loss: 0.2973 - val_acc: 0.8818\n",
      "Epoch 29/30\n",
      "18819/18819 [==============================] - 2s 114us/step - loss: 0.2902 - acc: 0.8834 - val_loss: 0.2891 - val_acc: 0.8852\n",
      "Epoch 30/30\n",
      "18819/18819 [==============================] - 2s 107us/step - loss: 0.2843 - acc: 0.8845 - val_loss: 0.2820 - val_acc: 0.8863\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = ['acc'])\n",
    "#ada grad\n",
    "#ada_grad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "#ada_delta\n",
    "#ada_delta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "#Adam\n",
    "#adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "\n",
    "#ada_max\n",
    "ada_max = keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = ada_max, metrics = ['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 30, batch_size = 1000, \n",
    "                   validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18819 samples, validate on 4705 samples\n",
      "Epoch 1/30\n",
      "18819/18819 [==============================] - 3s 180us/step - loss: 0.5479 - acc: 0.8198 - val_loss: 0.3898 - val_acc: 0.8635\n",
      "Epoch 2/30\n",
      "18819/18819 [==============================] - 2s 107us/step - loss: 0.3885 - acc: 0.8596 - val_loss: 0.3466 - val_acc: 0.8693\n",
      "Epoch 3/30\n",
      "18819/18819 [==============================] - 2s 103us/step - loss: 0.3685 - acc: 0.8650 - val_loss: 0.3513 - val_acc: 0.8725\n",
      "Epoch 4/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.3479 - acc: 0.8682 - val_loss: 0.3284 - val_acc: 0.8710\n",
      "Epoch 5/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3515 - acc: 0.8670 - val_loss: 0.3341 - val_acc: 0.8710\n",
      "Epoch 6/30\n",
      "18819/18819 [==============================] - 2s 104us/step - loss: 0.3326 - acc: 0.8728 - val_loss: 0.3271 - val_acc: 0.8616\n",
      "Epoch 7/30\n",
      "18819/18819 [==============================] - 2s 108us/step - loss: 0.3239 - acc: 0.8723 - val_loss: 0.3057 - val_acc: 0.8748\n",
      "Epoch 8/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.3233 - acc: 0.8754 - val_loss: 0.3003 - val_acc: 0.8816\n",
      "Epoch 9/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3147 - acc: 0.8758 - val_loss: 0.3199 - val_acc: 0.8799\n",
      "Epoch 10/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3207 - acc: 0.8767 - val_loss: 0.3039 - val_acc: 0.8769\n",
      "Epoch 11/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.3073 - acc: 0.8785 - val_loss: 0.2907 - val_acc: 0.8825\n",
      "Epoch 12/30\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 0.3089 - acc: 0.8803 - val_loss: 0.2922 - val_acc: 0.8844\n",
      "Epoch 13/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.3062 - acc: 0.8781 - val_loss: 0.3022 - val_acc: 0.8842\n",
      "Epoch 14/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.3024 - acc: 0.8799 - val_loss: 0.3019 - val_acc: 0.8820\n",
      "Epoch 15/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.2986 - acc: 0.8812 - val_loss: 0.2859 - val_acc: 0.8846\n",
      "Epoch 16/30\n",
      "18819/18819 [==============================] - 2s 104us/step - loss: 0.3026 - acc: 0.8799 - val_loss: 0.2864 - val_acc: 0.8842\n",
      "Epoch 17/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.2997 - acc: 0.8824 - val_loss: 0.3229 - val_acc: 0.8850\n",
      "Epoch 18/30\n",
      "18819/18819 [==============================] - 2s 107us/step - loss: 0.2944 - acc: 0.8846 - val_loss: 0.2847 - val_acc: 0.8850\n",
      "Epoch 19/30\n",
      "18819/18819 [==============================] - 2s 106us/step - loss: 0.2975 - acc: 0.8830 - val_loss: 0.2829 - val_acc: 0.8871\n",
      "Epoch 20/30\n",
      "18819/18819 [==============================] - 2s 109us/step - loss: 0.2985 - acc: 0.8833 - val_loss: 0.2843 - val_acc: 0.8865\n",
      "Epoch 21/30\n",
      "18819/18819 [==============================] - 2s 116us/step - loss: 0.3231 - acc: 0.8802 - val_loss: 0.3439 - val_acc: 0.8820\n",
      "Epoch 22/30\n",
      "18819/18819 [==============================] - 2s 108us/step - loss: 1.7171 - acc: 0.8248 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 23/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 24/30\n",
      "18819/18819 [==============================] - 2s 117us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 25/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 26/30\n",
      "18819/18819 [==============================] - 2s 113us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 27/30\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 28/30\n",
      "18819/18819 [==============================] - 2s 112us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 29/30\n",
      "18819/18819 [==============================] - 2s 111us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n",
      "Epoch 30/30\n",
      "18819/18819 [==============================] - 2s 113us/step - loss: 2.2868 - acc: 0.8581 - val_loss: 2.1993 - val_acc: 0.8635\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = ['acc'])\n",
    "#ada grad\n",
    "#ada_grad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "#ada_delta\n",
    "#ada_delta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "#Adam\n",
    "#adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "\n",
    "#ada_max\n",
    "#ada_max = keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "\n",
    "#nadam\n",
    "nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = nadam, metrics = ['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 30, batch_size = 1000, \n",
    "                   validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the training and validation loss\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, 31)\n",
    "plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label = 'validation loss')\n",
    "plt.title('training and validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the training and validation acc\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['acc']\n",
    "val_loss_values = history_dict['val_acc']\n",
    "\n",
    "epochs = range(1, 31)\n",
    "plt.plot(epochs, loss_values, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_loss_values, 'b', label = 'validation acc')\n",
    "plt.title('training and validation acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4705/4705 [==============================] - 1s 127us/step\n",
      "18819/18819 [==============================] - 2s 125us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.28423121379615141, 0.88735387885228478],\n",
       " [0.2772710949389966, 0.88724161751738162])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test), model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    1\n",
       "8    0\n",
       "9    0\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(read_test)\n",
    "set_val = 0\n",
    "\n",
    "def approximate(val):\n",
    "    if val <= 0.5:\n",
    "        set_val = 0\n",
    "    else:\n",
    "        set_val = 1\n",
    "    return set_val\n",
    "data = pd.DataFrame(prediction)\n",
    "approximate_data = data[0].apply(approximate)\n",
    "approximate_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uniqueid_6056 x Kenya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uniqueid_6060 x Kenya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uniqueid_6065 x Kenya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uniqueid_6072 x Kenya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uniqueid_6073 x Kenya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                uniqueid\n",
       "0  uniqueid_6056 x Kenya\n",
       "1  uniqueid_6060 x Kenya\n",
       "2  uniqueid_6065 x Kenya\n",
       "3  uniqueid_6072 x Kenya\n",
       "4  uniqueid_6073 x Kenya"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id = pd.read_csv('test_unique_id.csv')\n",
    "unique_id = unique_id.drop('Unnamed: 0', axis = 1)\n",
    "unique_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueid</th>\n",
       "      <th>bank_account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uniqueid_6056 x Kenya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uniqueid_6060 x Kenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uniqueid_6065 x Kenya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uniqueid_6072 x Kenya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uniqueid_6073 x Kenya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                uniqueid  bank_account\n",
       "0  uniqueid_6056 x Kenya             0\n",
       "1  uniqueid_6060 x Kenya             1\n",
       "2  uniqueid_6065 x Kenya             0\n",
       "3  uniqueid_6072 x Kenya             0\n",
       "4  uniqueid_6073 x Kenya             0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id['bank_account'] = approximate_data\n",
    "unique_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueid</th>\n",
       "      <th>bank_account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uniqueid_6056 x Kenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uniqueid_6060 x Kenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uniqueid_6065 x Kenya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uniqueid_6072 x Kenya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uniqueid_6073 x Kenya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                uniqueid  bank_account\n",
       "0  uniqueid_6056 x Kenya             1\n",
       "1  uniqueid_6060 x Kenya             1\n",
       "2  uniqueid_6065 x Kenya             0\n",
       "3  uniqueid_6072 x Kenya             0\n",
       "4  uniqueid_6073 x Kenya             0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_data = pd.read_csv('gbrt.csv')\n",
    "#gbrt_data = gbrt_data.drop('Unnamed: 0', axis = 1)\n",
    "gbrt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94140392623438429"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(gbrt_data['bank_account'], unique_id['bank_account'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_id.to_csv('keras_one.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
