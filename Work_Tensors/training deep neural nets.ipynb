{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gradient often get smaller and smaller as the algorithm progresses down to the lower laye. As a result, the gradient descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the varnishing gradients problem.\n",
    "\n",
    "In some cases, we have the opposite happening, the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is exploding gradient problem which is mostly encountered in recurrent neural networks. \n",
    "\n",
    "More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "\n",
    "using the Xavier initialization strategy can speed up training considerably, and it is one of the tricks that led to the current success of deep learning.\n",
    "\n",
    "He intialization considers only the fan-in, not the average between fan-in and fan-out like in Xavier intialization. This is also the default for the variance_scaling_intializer() function.\n",
    "\n",
    "ReLU, LeakyReLU, randomized leaky ReLU(RReLU), parametric leaky ReLU(PReLU), ELUs(exponential linear unit).\n",
    "\n",
    "\n",
    "using the He initialization along with ELU (or any variant of ReLU) can significantly reduce the vanishing/exploding gradients problems at the beginning of training. It doesn't guarantee that they won't come back during training.\n",
    "\n",
    "Gradient clipping: A popular technique to lessen the exploding gradient problem is to simply clip the gradients problem during backpropagation so that they never exceed some threshold (this is mostly useful for recurrent neural networks).\n",
    "In general, people prefer batch normalization, but its useful to know about gradient clipping and how to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, the tf.layers.dense() function uses Xavier intialization (with a unform distribution)\n",
    "# it could be changed to He intialization by using the variance_scaling_intializer()\n",
    "\n",
    "import tensorflow as tf\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, \n",
    "                         kernel_intializer = he_init, name = 'hidden1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using the elu as activation function for a simple neural net\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.elu, name = 'hidden1')\n",
    "\n",
    "\n",
    "def leaky_relu(z, name = None):\n",
    "    return tf.maximum(0.01 *z, z, name = name)\n",
    "# using the leaky_relu as activation function for a simple neural net\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = leaky_relu, name = 'hidden1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implementing Batch Normalization in tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "n_inputs= 28* 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape =(None,n_inputs), name = 'X')\n",
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name = 'hidden1')\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = 'hidden2')\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training = training, momentum = 0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name = 'outputs')\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, tranining, momentum = 0.9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training = training, momentum = 0.9)\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name = 'hidden1')\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = 'hidden2')\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training = training, momentum = 0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name = 'outputs')\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, tranining, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                    feed_dict= {training: True, X: X_batch, y:y_batch})\n",
    "            accuracy_val = accuracy.eval(feed_dict = {X: mnist.test.images, \n",
    "                                                     y:mnist.test.labels})\n",
    "            print(epoch, 'test accuracy: ',accuracy_val)\n",
    "            \n",
    "            \n",
    "        save_path = saver.save(sess, './my_model_final.ckpt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient clipping\n",
    "#The optimizer's minimize() function takes care of both computing the gradients and applying them,\n",
    "# so you must instead call the optimizer's compute_gradients() method first, then\n",
    "# create an operation to clip the gradients using the clip_by_value() function, and \n",
    "# finally create an operation to apply the clipped gradients using the optimizer's \n",
    "# apply_gradients() method\n",
    "\n",
    "threshold = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning - Reusing pretrained layers\n",
    "# implies using existing neural network that accomplishes a similar task to the one\n",
    "# you are trying to tackle, then just reuse the lower layers of this network: this is\n",
    "# called transfer learning. It will not only speed up training considerably, but will\n",
    "# also require much less training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusing a tensorflow model\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name('eval/accuracy:0')\n",
    "training_op = tf.get_default_graph().get_operation_by_name('GradientDescent')\n",
    "\n",
    "\n",
    "# if the pretrained model is not well documented, then you will have to explore the\n",
    "# graph to find the names of the operations you will need.\n",
    "\n",
    "for op in tf.get_default_graph().get_operation():\n",
    "    print(op.name)\n",
    "    \n",
    "# a way to create a collection containing all the important operations that people \n",
    "# will want to get a handle on is:\n",
    " for op in (X, y, accuracy, training_op):\n",
    "        tf.add_to_collection('my_important_ops', op)\n",
    "        \n",
    "# this way people who reuse your model will be able to simply write:\n",
    "X, y, accuracy, training_op = tf.get_collection('my_important_ops')\n",
    "\n",
    "\n",
    "# we can then restore the model state using the Saver and continue training\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_model_final.ckpt')\n",
    "    [....] # train the model on your own data\n",
    "\n",
    "# import_meta_graph() will load the entire original graph    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
