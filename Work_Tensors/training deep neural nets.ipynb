{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gradient often get smaller and smaller as the algorithm progresses down to the lower laye. As a result, the gradient descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the varnishing gradients problem.\n",
    "\n",
    "In some cases, we have the opposite happening, the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is exploding gradient problem which is mostly encountered in recurrent neural networks. \n",
    "\n",
    "More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "\n",
    "using the Xavier initialization strategy can speed up training considerably, and it is one of the tricks that led to the current success of deep learning.\n",
    "\n",
    "He intialization considers only the fan-in, not the average between fan-in and fan-out like in Xavier intialization. This is also the default for the variance_scaling_intializer() function.\n",
    "\n",
    "ReLU, LeakyReLU, randomized leaky ReLU(RReLU), parametric leaky ReLU(PReLU), ELUs.\n",
    "\n",
    "\n",
    "using the He initialization along with ELU (or any variant of ReLU) can significantly reduce the vanishing/exploding gradients problems at the beginning of training. It doesn't guarantee that they won't come back during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by default, the tf.layers.dense() function uses Xavier intialization (with a unform distribution)\n",
    "# it could be changed to He intialization by using the variance_scaling_intializer()\n",
    "\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_intializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu, \n",
    "                         kernel_intializer = he_init, name = 'hidden1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using the elu as activation function for a simple neural net\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.elu, name = 'hidden1')\n",
    "\n",
    "\n",
    "def leaky_relu(z, name = None):\n",
    "    return tf.maximum(0.01 *z, z, name = name)\n",
    "# using the leaky_relu as activation function for a simple neural net\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation = leaky_relu, name = 'hidden1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Normalization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
