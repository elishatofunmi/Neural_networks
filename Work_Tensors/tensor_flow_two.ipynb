{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review =' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_1 = np.zeros((len(train_data[:10000]), 10000))\n",
    "result_2 = np.zeros((15000, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_1_pd = pd.DataFrame(result_1)\n",
    "result_2_pd = pd.DataFrame(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_with_ones(sequences, dimension = 10000):\n",
    "    return np.ones((len(sequences), dimension))\n",
    "x_train = vectorize_with_ones(train_data)\n",
    "x_test = vecotorize_with_ones(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    result = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        result[i, sequence] = 1\n",
    "    return result\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output= relu(dot(weight, input) + b)\n",
    "having 16 hidden units means the weight (W) will have shape (input_dimension, 16): the dot product with W will project the input data onto a 16-dimensional representation space(and then you'll add the bias vector b and apply the relu operation).\n",
    "\n",
    "Having more hidden units (a higher-dimensional representation space allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns (patterns that will improve on the training data but not on the test data).\n",
    "\n",
    "# How many layers to use\n",
    "# how many hidden units to choose for each layer.\n",
    "\n",
    "A relu is a function meant to zero out negative values whereas a sigmoid \"Squashes\" arbitrary value into the [ 0,1] interval, outputing something that can be interpreted as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the model defination\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "finally we need to choose a loss function and an optimizer. Because we are facing a binary classification problem and the output of the network is a probability (the network is ended with a single-unit layer with a sigmoid activation), it's best to use the binary_crossentropy loss. It isn't the only viable choice: you could use, for instance, mean_squared_error. But crossentropy is usually the best choice when you're dealing with models that output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuring the optimizer\n",
    "\n",
    "from keras import optimizers\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.001), loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using custom losses and metrics\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.001), loss = losses.binary_crossentropy,\n",
    "             metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validating your data\n",
    "#x_val = x_train[:10000]\n",
    "#partial_x_train = x_train[10000:]\n",
    "#y_val = y_train[:10000]\n",
    "#partial_y_train = y_train[1000:]\n",
    "\n",
    "# training your model\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "\n",
    "#history = model.fit(partial_x_train, partial_y_train, epochs = 20, batch_size = 512, validation_data = [x_val, y_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history\n",
    "#history_dict = history.history\n",
    "#history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting the training and validation loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#history_dict = history.history\n",
    "#loss_values = history_dict['loss']\n",
    "#val_loss_values = history_dict['val_loss']\n",
    "\n",
    "#epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#plt.plot(epochs, loss_values, 'bo', label = 'training loss')\n",
    "#plt.plot(epochs, val_loss_values, 'b', label = 'validation loss')\n",
    "#plt.title('Traning and validation loss')\n",
    "#plt.xlabel('epochs')\n",
    "#plt.ylabel('loss')\n",
    "#plt.legend()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting the training and validation accuracy\n",
    "\n",
    "#plt.clf()# clear figure\n",
    "#acc_values = history_dict['acc']\n",
    "#val_acc_values = history_dict['val_acc']\n",
    "\n",
    "#plt.plot(epochs, acc, 'bo', label = 'Traning acc')\n",
    "#plt.plot(epochs, val_acc, 'b', label = 'validation acc')\n",
    "#plt.title('Training and validation accuracy')\n",
    "#plt.xlabel('Epochs')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 2246)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoding newswires back to text\n",
    "\n",
    "word_index = reuters.get_word_index()\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding the data\n",
    "\n",
    "#x_train = vectorize_sequences(train_data)\n",
    "#x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension = 46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1\n",
    "    return results\n",
    "\n",
    "#one_hot_train_labels = to_one_hot(train_labels)\n",
    "#one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy and reshuffle data\n",
    "\n",
    "import copy\n",
    "#test_labels_copy = copy.copy(test_labels)\n",
    "#np.random.shuffle(test_labels_copy)\n",
    "#hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
    "#float(np.sum(hits_array)) / len(test_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if you're trying to classify data points among N classes, your network should end with a dense layer of size N.\n",
    "\n",
    "In a single-label, multiclass classification problem, your network should end with a softmax activation so that it will output a probability distribution over the N output classes.\n",
    "\n",
    "Categorical crossentropy is almost always the loss function you should use for such problems. It minimizes the distance between the probility distributions output by the network and the true distribution of the targets.\n",
    "\n",
    "There are two ways to handle labels in multiclass classification:\n",
    "- Encoding the labels via categorical encoding (also known as one-hot encoding) and using categorical_crossentropy as a loss function.\n",
    "- Encoding the labels as integers and using the sparse_categorical_crosentropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting house price a regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (102, 13))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "mean = train_data.mean(axis = 0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis = 0)\n",
    "train_data/= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data/= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model defination \n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
    "    \n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mse'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validating your data using k-fold validation\n",
    "\n",
    "import numpy as np\n",
    "#k = 4\n",
    "\n",
    "#num_val_samples = len(train_data) // k\n",
    "#num_epochs = 100\n",
    "#all_scores =[]\n",
    "\n",
    "#for i in range(k):\n",
    "#    print(\"processing fold #\", i)\n",
    "#    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "#    val_targets = train_target[i * num_val_samples: (i +1)  * num_val_samples]\n",
    "#    \n",
    "#    partial_train_data = np.concatenate([train_data[:i * num_val_samples],\n",
    "#                                        train_data[(i+1) * num_val_samples:]],\n",
    "#                                       axis = 0)\n",
    "##    \n",
    "#    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],\n",
    "#                                           train_targets[(i+1) * num_val_samples:]],\n",
    "#                                          axis = 0)\n",
    "#    \n",
    "#    model = build_model()\n",
    "#    model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 1,\n",
    "#             verbose = 0)\n",
    "#    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose = 0)\n",
    "#    all_scores.append(val_mae)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the validation logs at each fold\n",
    "\n",
    "num_epoch = 500\n",
    "all_mae_histories = []\n",
    "#for i in range(k):\n",
    "#    print('processing fold #', i)\n",
    "#    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "#    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "#    partial_train_data = np.concatenate([train_data[:i * num_val_samples],\n",
    "#                                        train_data[(i+1) * num_val_samples:]],\n",
    "#                                       axis = 0)\n",
    "##    \n",
    "#    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],\n",
    "#                                           train_targets[(i+1) * num_val_samples:]],\n",
    "#                                          axis = 0)\n",
    "#    \n",
    "#    model = build_model()\n",
    "#    model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 1,\n",
    "#             verbose = 0)\n",
    "#    mae_history = history.history['val_mean_absolute_error']\n",
    "#    all_mae_histories.append(mae_history)\n",
    "#    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
