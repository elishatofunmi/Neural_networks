{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The perceptron is one of the simplest ANN architectures, invented by Frank Rosenblatt in 1957, it is based on artificial Neuron called a linear threshold unit(LTU).\n",
    "The most common step function used in Perceptron is the heavyside step function.\n",
    "The decision boundary of each output neuron is linear, so perceptrons are incapable of learning complex patterns(just like logistic regression classifiers).\n",
    "Scikit-learn's perceptron class is equivalent to using an SGDClassifier witht the following hyperparameters: loss = 'perceptron', learning_rate = 'constant', eta0 = 1, penalty = None\n",
    "\n",
    "Note that contrary to logistic Regression classifiers, Perceptrons do not output a class probability; rather, they just make predictions based on hard threshold. This is one of the good reasons to prefer Logistic Regression over perceptrons.\n",
    "\n",
    "However, it turns out that some of the limitations of perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a Multi-layer perceptron (MLP).\n",
    "\n",
    "An MLP is composed of one(passthrough) input layer, one or more layers of LTUs called hidden layers, and one final layer of LTUs called the output layer. Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called a DEEP NEURAL NETWORK(DNN).\n",
    "\n",
    "Backpropagation training algorithm is described as gradient descent using reverse-mode autodiff.\n",
    "1. it feeds training instances to the network and computes the output of every neuron in each consective layer. \n",
    "2. measures the network's output error(i.e., the difference between the desired output and the actual output of the network).\n",
    "3. computes how much each neuron in the last hidden layer contributed to each output neuron's error.\n",
    "4. measures how much of these error contributions came from each neuron in the previous hidden layer and so on until the algorithm reaches the input layer.\n",
    "\n",
    "The last step of the backpropagation algorithm is a gradient descent step on all the connection weights in the network, using the error gradients measured earlier.\n",
    "In summary, for each training instance the backpropagation algorithm first makes a prediction(forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection(reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step).\n",
    "\n",
    "activation algorithms(step function) include gradient descent, logistic function, hyperbolic tangent function tanh(z) = z(sig)(2z) - 1\n",
    "Relu function - RELU(z) = max(0,z)\n",
    "\n",
    "Feedforward Neural Network (the signal flows on ly in one direction; from the inputs to the outputs) - FNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "iris= load_iris()\n",
    "x = iris.data[:, (2,3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # iris setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_clf = Perceptron(random_state = 42)\n",
    "per_clf.fit(x, y)\n",
    "y_pred = per_clf.predict([[2,0.5], [1,2], [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=1, fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "       loss='perceptron', n_iter=5, n_jobs=1, penalty=None, power_t=0.5,\n",
       "       random_state=None, shuffle=True, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the stochastic gradient descent algorithm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss = 'perceptron', learning_rate = 'constant', eta0 = 1, penalty = None)\n",
    "sgd.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = sgd.predict([[2,0.5],[1,2],[3,4]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training an MLP with TensorFlow's high-level API on the Mnist image dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "feature_cols= tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units = [300,100], n_classes = 10, \n",
    "                                        feature_columns = feature_cols)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if Tensorflow >= 1.1\n",
    "dnn_clf.fit(X_train, y_train, batch_size = 50, steps = 40000)\n",
    "\n",
    "\n",
    "from sklearn.metrics import  accuracy_score\n",
    "y_pred = dnn_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function truncated_normal in module tensorflow.python.ops.random_ops:\n",
      "\n",
      "truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
      "    Outputs random values from a truncated normal distribution.\n",
      "    \n",
      "    The generated values follow a normal distribution with specified mean and\n",
      "    standard deviation, except that values whose magnitude is more than 2 standard\n",
      "    deviations from the mean are dropped and re-picked.\n",
      "    \n",
      "    Args:\n",
      "      shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
      "      mean: A 0-D Tensor or Python value of type `dtype`. The mean of the\n",
      "        truncated normal distribution.\n",
      "      stddev: A 0-D Tensor or Python value of type `dtype`. The standard deviation\n",
      "        of the truncated normal distribution.\n",
      "      dtype: The type of the output.\n",
      "      seed: A Python integer. Used to create a random seed for the distribution.\n",
      "        See\n",
      "        @{tf.set_random_seed}\n",
      "        for behavior.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A tensor of the specified shape filled with random truncated normal values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "help (tf.truncated_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training a DNN using plain tensorflow\n",
    "n_inputs = 28 *28 # MNIST no features is 28 by 28(one feature per pixel)\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "# since we do not know how many instances each training batch will contain, so the shape\n",
    "# of X is (None, n_inputs) and y is (None)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name= 'y')\n",
    "\n",
    "# creating the two hidden layers and the output, the output layer uses a softmax activation\n",
    "# function instead of a RELU activation function.\n",
    "# we will create a nuuron_layer() function that we will use to create one layer at a time\n",
    "# It will need parameters to specify the inputs, the number of neutrons, the activation function,\n",
    "# and the name of the layer....\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation = None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2/np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons),stddev = stddev)\n",
    "        W = tf.Variable(init,name = 'kernel')\n",
    "        b = tf.Variable(tf.zeros(n_neurons), name = 'bias')\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation is not None: # that is if activation parameter is provided\n",
    "            # such as tf.nn.relu(i.e, max(0,z)), then the code returns activation(Z), or\n",
    "            #else it just returns Z.\n",
    "            return activation(z)\n",
    "        else:\n",
    "            return z\n",
    "        \n",
    "        \n",
    "        \n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name = 'hidden1', activation = tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name = 'hidden2', activation = tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name = 'outputs')\n",
    "    \n",
    "    \n",
    "# using tensorflow built in support for building layers\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name = 'hidden1', activation = tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(x, n_hidden2, name = 'hidden2', activation = tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name = 'outputs')\n",
    "    \n",
    "    \n",
    "# applyiing cross Entropy, cross entropy will penalize models that estimates a low\n",
    "# probability for the target class. using sparse_softmax_cross_entropy_with_logits()\n",
    "# it computes the cross entropy based on the 'logits' (i.e., the output of the network)\n",
    "# before going through the softmax activation function), and it expects labels in the form\n",
    "# of integers ranging from 0 to the number of classes minus 1(in our case, from 0 to 9). This\n",
    "# will give us a 1D tensor containing the cross entropy for each instance. \n",
    "# we can then use Tensorflow's reduce_mean() function to compute the mean cross entropy over all\n",
    "# instances.\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now getting the GradientDescentOptimizer that will tweak the model parameters, to\n",
    "# minimize the cost function\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    trainin_op = optimizer.minimize(loss)\n",
    "    \n",
    "    \n",
    "# now to specify how to evaluate our model, simply using accuracy as our performance measure\n",
    "# to determine if the neural network's prediction is correct by checking whether or not\n",
    "# the highest logit corresponds to the target class... using the in_to_k() function.\n",
    "# this returns a 1D tensor full of boolean values, so we need to cast these booleans to floats\n",
    "# and the compute the average.\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorial.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as ess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X:X_batch, y:y_batch})\n",
    "            acc_train = accuracy.eval(feed_dict = {X:X_batch, y:y_batch})\n",
    "            acc_val = accuracy.eval(feed_dict = {X:mnist.validation.images, y:mnist.validation.labels})\n",
    "            print(epoch, 'Train accuracy:', acc_train, 'Val accuracy:', acc_val)\n",
    "            \n",
    "        save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_model_final.ckpt')\n",
    "    X_new_scaled = [....] # some new images(scaled from 0 to 1)\n",
    "    z = logits.eval(feed_dict = {X:X_new_scaled})\n",
    "    y_pred = np.argmax(z, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fine-tuned neural network hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# for the output layer, the softmax activation function is generally a good choice\n",
    "# for classification tasks when the classes are mutually exclusive. When they are not\n",
    "# mutally exclusive(or when they are just two classes), you generally want to use the \n",
    "# logistic function. For regression tasks, you can simply use no activation function at all\n",
    "# for the output layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
